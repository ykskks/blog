---
title: "はじパタを読んだ時のメモ"
date: 2020-08-25
tags: ["機械学習"]

---
はじパタを読んだので気になったところだけ自分的メモを適当にまとめる。

## 1.3 特徴ベクトル空間と次元の呪い
「次元の呪い」、よく言葉は聞くけどいまいち理解できてないやつ。

とりあえず言葉自体の意味は
> 未知の複雑な関数を学習するために必要なデータが、次元の増加と共に指数関数的に増加すること

らしい

機械学習だと、次元が増えるにつれ、入力のパターンも指数関数的に増えるから、学習に必要なデータも増加するみたいな感じか。まあ当たり前な感じもする。

直感的には、次元が増えれば増えるほど、近くにあるデータ点が少なくなる傾向にあるみたい。

この辺の話も関係あるのかな。
[https://shuyo.hatenablog.com/entry/20160401/cosine_similarity](https://shuyo.hatenablog.com/entry/20160401/cosine_similarity)

## 2.2 汎化能力
汎化性能の評価方法について

- ホールドアウト法  
    - 学習データを多くすればテストデータが減り学習精度は良くなるが性能評価の精度は下がる、学習データを減らしテストデータを増やすと学習の精度が悪くなる可能性がある。
    - つまり、学習・テスト用両方のデータを十分に取れる、手元のデータが大量にある場合のみ適切。
    
    画像とかの深層学習だと基本ホールドアウト使ってる気がするな（計算コスト的にクロスバリデーションが無理というのもあるが）

- 交差検証法
- leave-one-out法
- bootstrap法  
    - ブートストラップサンプルを用いて推定したバイアスを用いて、再代入謝り率を修正する。

    学習データでテストすると、未知データでテストした時よりも誤差が小さくなるが、その差（バイアスと呼ぶ）を推定することで、本当に知りたい汎化誤差に近い値が得られるように修正する、という方針なのだけど

    なんでわざわざこんなめんどくさいことするのかな。ちゃんと理由があると思うのだけど

## 4.1 観測データの線形変換
- 標準化
    - 平均ゼロ、分散１に
- 無相関化
    - 主成分分析と密接に関係している
    - 元データの共分散行列の固有ベクトルを縦に並べた行列$$S$$を用いて$$y = S^{\mathrm{T}} x$$のように元データを線形変換すると、変換後のデータの共分散行列は対角行列であり、無相関化されている。
- 白色化
    - 無相関化に加え、正規化と中心化を加える操作。$$u = \Lambda^\frac{-1}{2} S^{\mathrm{T}} (x-\mu)$$という変換を行うことで得られる。なお、$$\Lambda$$は元データの共分散行列を対角化したもの。

## 4.3 確率モデルのパラメータ推定
- まずモデルを仮定し、そのパラメータをデータを用いて推定する。
- 最尤推定法はその一つの方法である。

## 5.1 最近傍法とボロノイ境界
- 入力データ$$x$$を学習データの中で最もユークリッド距離が小さいデータのクラスに分類する。
- 手書き文字認識の例で考えると、著しく位置ずれがある場合たとえ文字の形は類似していても誤分類されがちである。
    - CNNとかだとプーリングとかでこの辺のズレを吸収してるんだなぁ
- 基本的に学習データが多いほど精度は上がる

## 5.2 kNN法
- 最近傍だけでなく、近傍のk個を取ってきて多数決する。
- 最近傍法に比べて、識別境界は滑らかである。
- 入力の次元が高い場合には、**次元の呪いにより入力データから同じくらいの距離の位置に異なるクラスのデータが存在する**ため、kを増やすことでかえって精度が悪化することがある。（上の手書き文字認識など）

## 5.4 kNN法の計算量とその低減法
kNNはデータの次元数や学習データ数が増えるにつれ、識別に必要な時間とメモリが増大する。そこでこれらを解消するためのいくつかの改良方が提案されている。  

- 誤り削除型
- 圧縮型
- 分枝限定法
- 近似最近傍探索

## 6.4 ロジスティック回帰
- 一般化線形モデル
- $$\ln{\frac{P(C1\mid x)}{P(C2\mid x)}}$$を説明変数の線形和でモデリングする
- さらに$$\ln{\frac{P(C1\mid x)}{P(C2\mid x)}}$$をロジスティック関数で0~1に変換して事象を表現している
- 識別境界は線形
- 係数が$$w_1$$のとき、**対応する変数が一単位増えた時、オッズは$$\exp(w_1)$$倍**になる。
- パラメータの最尤推定
    - クロスエントロピーロスを最小化する$$w$$を求める。
    - 解析的に解を求められないので、最急降下法などで数値的に求める。

## 7.2 誤差逆伝播法
タイトルは誤差逆伝播法となっているが、内容は多層パーセプトロンの導入を含む。

- 活性化関数は通常非線形の物を使う。線形なら多層回路を構成しても等価な単層回路で表現できてしまうので
- sigmoidの場合、有名な勾配消失とか

## 9.2 主成分分析
- 分散最大となるような線形変換を求める。
- ラグランジュ未定乗数法により、そのような線形変換は元データの共分散行列の固有ベクトルによる線形変換であることがわかる。
- 共分散行列は実対称行列なので、固有ベクトルは互いに直交する。
- 線形変換後の分散は固有値に一致し、最大固有値に対応する方向が第一主成分。

## 10.2 非階層型クラスタリング（K-平均法）
- データ間の類似性（距離）を尺度に、**予め定めた**K個のクラスタに分類する。
- 各サンプルの重心との距離を最小にするように、クラスターの割り当てと重心を逐次的に最適化する。
- 初期値として選ぶ重心に最終結果が依存するため、より最適解に近い結果を得るには**初期値を変えて複数回試行し、最も指標が良いものを採用する**などする必要がある。

## 10.3 階層的クラスタリング
K-平均法などと違い、事前にクラスター数を決めずに、類似度の高いものを順番に融合して大きなクラスターを作っていき、最終的に一つのクラスタに達する。
クラスタ間の類似度をどう定義するかによって、複数の手法が存在し、

- 単連結法（クラスタ間で最も類似度の高いデータ間の類似度をクラスタ間の類似度とする。）
- 完全連結法（最も類似度の低いデータ間の類似度）
- 群平均法
- Ward法

などがある。

単連結法では大きなクラスタができやすく、完全連結法では同じくらいのあまり大きくないクラスタができやすい。
階層法の中では、Ward法が**最も精度が高い**らしい。

## 10.4 確率モデルによるクラスタリング
データ分布を複数の確率モデルの重み付き線形和でモデリングする。代表的なものとして混合正規分布。
以下は混合正規分布の話。

- データ$$x$$に対して、推定するのは各分布の混合比、平均ベクトル、共分散行列。
- これらを推定するために、一つのデータがどのクラスタに属するかを表現する隠れ変数を導入する。
- 隠れ変数がある場合に、確率モデルのパラメータの最尤推定値を求める優れた手法である**EMアルゴリズム**を用いる。
    - Eステップでは、パラメータを固定して、隠れ変数の事後確率を求める。
    - Mステップでは、隠れ変数の事後確率を用いて、パラメータを更新する。
    - これを対数尤度が変化しなくなるまで繰り返す。

ここはまだあまり理解できていない

## 11.2 決定木
決定木では、不純度と呼ばれる指標を定義し、この値が最も減少するような変数の閾値で分岐を行い、データを分割していく。不純度の定義の代表的なものとしては、

- 誤り率
- 交差エントロピー
- ジニ係数 

などがある。

この不純度の減少量を変数毎に森全体で平均したものが、いわゆる**特徴量重要度**である。

## 11.3 バギング
ブートストラップサンプルを用いて複数の学習器を学習し、それらの多数決で予測を行う。

識別器が持つばらつきはブートストラップサンプルのばらつきが反映されるだけなので、各決定木間の相関が高くなり、十分に性能が強化できない可能性がある。

## 11.4 アダブースト
- 決定木を直列に並べる
- 各学習器でその前の学習器がうまく分類できなかったサンプルの重みを大きくして学習する。
- 最終的な予測は、各識別器の重み付き線形和である。重みは各識別器の誤差関数の値をもとに計算される。

## 11.5 ランダムフォレスト
バギングを改良し、各決定木をブートストラップサンプルで学習するのに加えて、各ノードで分岐に用いる変数をランダムに選択することで、相関の低い多様な決定木を学習し、よりアンサンブルの効果を強化したもの。












